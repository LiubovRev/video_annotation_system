# =============================================================================
# config.yaml
# Full Pipeline Configuration
# Edit this file to configure paths, flags, and project parameters.
# =============================================================================

# -----------------------------------------------------------------------------
# Directories
# -----------------------------------------------------------------------------
directories:
  base_data_dir:      "/home/liubov/Bureau/new/processed_data"
  output_base_dir:    "/home/liubov/Bureau/new/output_data"
  annotations_dir:    "/home/liubov/Bureau/new/new_annotations"
  # Raw video projects root — each subfolder is one project
  raw_video_root:     "/home/liubov/Bureau/new"

# -----------------------------------------------------------------------------
# Pipeline flags
# Set to true to skip a stage if its output already exists (speeds up reruns).
# -----------------------------------------------------------------------------
flags:
  skip_video_processing:      false   # Skip Step 1 if processed_video.mp4 already exists
  skip_pose_extraction:       false   # Skip Step 2 if processed_data.csv already exists
  skip_pose_clustering:       true    # Skip Step 3 (optional) — set false to enable
  skip_annotation_processing: true    # Skip Step 4 if combined file exists
  force_recombine:            false   # Force re-combining all labeled datasets

# -----------------------------------------------------------------------------
# Video processing (Step 1 — src/video_processing/processing.py)
# -----------------------------------------------------------------------------
video_processing:
  raw_video_filename: "camera_a.mkv"  # Expected filename inside each project folder
  fps:                15
  step_size:          1
  nb_objects:         "2"
  yolo_model:         "yolo11m.pt"
  model_size:         "small"
  object_class:       "0"
  device:             "cuda"

# -----------------------------------------------------------------------------
# Pose extraction (Step 2 — src/pose/extractor.py)
# -----------------------------------------------------------------------------
pose_extraction:
  directory:    "PosesDir"             # Subfolder inside each project dir with .tar.gz files
  abs_base:     0                      # Starting index offset for JSON renaming
  fps:          15
  id_to_label:                         # Maps tar filename prefix -> person label
    "1": "Therapist1"
    "2": "Patient1"


# -----------------------------------------------------------------------------
# Annotation alignment (Step 4 — src/annotations/generator.py)
# -----------------------------------------------------------------------------
annotation_alignment:
  # Subfolder inside each raw project dir that contains processed_data.csv
  features_subdir:    "PosesDir"
  # Column to drop after alignment (utility columns not needed for training)
  columns_to_drop:    ["time_min:s.ms"]
  # Person label inferred from annotation label prefix
  # "C*" -> Child, "T*" -> Therapist
  label_prefix_map:
    "C": "Child"
    "T": "Therapist"

# -----------------------------------------------------------------------------
# Pose clustering (optional Step 3 — src/pose/clustering.py)
# Set flags.skip_pose_clustering: true to bypass this step entirely.
# -----------------------------------------------------------------------------
pose_clustering:
  # Movement detection
  movement_threshold:   10       # px/frame — above this = "moving"
  # Proximity zones (pixels)
  proximity_close:      200
  proximity_medium:     400
  # Approach-response window
  response_window:      5        # seconds to check for child response after therapist approach
  # Data preprocessing
  confidence_threshold: 0.5
  smooth_window:        11
  smooth_poly:          3
  # Clustering k search range [start, stop] (exclusive stop, step 1)
  k_range:              [2, 8]
  # Session phase detection
  phase_window:         30       # seconds per window
  n_phases:             4
  # Person labels (must match person_label values in processed_data.csv)
  therapist_label:      "Therapist"
  child_label:          "Child"

# -----------------------------------------------------------------------------
# Project trim times
# Format per project: [start_sec, end_sec, fps]
# Use null for end_sec to trim to the end of the video.
# -----------------------------------------------------------------------------
trim_times:
  "11-1-2024_#9_INDIVIDUAL_[18]":   [165,  1930, 15]
  "23-5-2024_#20_INDIVIDUAL_[14]":  [500,  2595, 15]
  "14-3-2024_#15_INDIVIDUAL_[18]":  [165,  1860, 15]
  "11-1-2024_#7_INDIVIDUAL_[14]":   [170,  2160, 15]
  "10-1-2024_#6_INDIVIDUAL_[15]":   [210,  2940, 15]
  "15-5-2024_#20_INDIVIDUAL_[15]":  [0,    null,  15]

default_trim: [0, null, 15]

# -----------------------------------------------------------------------------
# Label map
# Numeric model output -> human-readable annotation code
# -----------------------------------------------------------------------------
label_map:
  0:  "C"
  1:  "CCR"
  2:  "CHO"
  3:  "CSI"
  4:  "CST"
  5:  "T"
  6:  "TC"
  7:  "TRE"
  8:  "TSI"
  9:  "TST"

# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------
model:
  file:          "model_xgboost.joblib"
  features_file: "feature_names.json"
  fps:           15

# -----------------------------------------------------------------------------
# Plotting
# -----------------------------------------------------------------------------
plotting:
  dpi:                 300
  figsize_per_project: [18, 10]
  figsize_global:      [24, 12]
  colormap:            "viridis"
