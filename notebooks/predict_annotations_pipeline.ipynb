{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================\n",
    "# User options\n",
    "# =========================\n",
    "BASE_DATA_DIR = Path(\"/home/liubov/Bureau/new/processed_data\")\n",
    "OUTPUT_BASE_DIR = Path(\"/home/liubov/Bureau/new/output_data\")\n",
    "MODEL_FILE = OUTPUT_BASE_DIR / \"/home/liubov/Bureau/new/output_data/model_xgboost.joblib\" \n",
    "FEATURES_FILE = OUTPUT_BASE_DIR / \"feature_names.json\"\n",
    "\n",
    "OUTPUT_BASE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Path to new data CSV for prediction\n",
    "NEW_DATA_FILE = BASE_DATA_DIR / \"processed_data14-3-2024_#15_INDIVIDUAL_[18].csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fd7777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from model_xgboost.joblib\n",
      "✓ Loaded 70 feature names\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Load model and features\n",
    "# =========================\n",
    "if not MODEL_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_FILE}\")\n",
    "\n",
    "model = joblib.load(MODEL_FILE)\n",
    "print(f\"✓ Loaded model from {MODEL_FILE.name}\")\n",
    "\n",
    "if not FEATURES_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Feature file not found: {FEATURES_FILE}\")\n",
    "\n",
    "with open(FEATURES_FILE, 'r') as f:\n",
    "    feature_names = json.load(f)\n",
    "print(f\"✓ Loaded {len(feature_names)} feature names\")\n",
    "\n",
    "# =========================\n",
    "# Load new data\n",
    "# =========================\n",
    "if not NEW_DATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {NEW_DATA_FILE}\")\n",
    "\n",
    "data = pd.read_csv(NEW_DATA_FILE)\n",
    "print(f\"✓ Loaded data: {NEW_DATA_FILE.name} ({len(data)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5231f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derived_pose_features(df):\n",
    "    \"\"\"\n",
    "    Add comprehensive joint angles, distances, and symmetry features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_angle(a, b, c):\n",
    "        \"\"\"Compute angle at point b formed by points a-b-c\"\"\"\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
    "        return np.arccos(np.clip(cosine_angle, -1.0, 1.0)) * (180.0 / np.pi)\n",
    "\n",
    "    def compute_distance(p1, p2):\n",
    "        \"\"\"Euclidean distance between two points\"\"\"\n",
    "        return np.linalg.norm(p1 - p2)\n",
    "\n",
    "    # === JOINT ANGLES ===\n",
    "    df['r_elbow_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['r_shoulder_x'], row['r_shoulder_y']]),\n",
    "        np.array([row['r_elbow_x'], row['r_elbow_y']]),\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_elbow_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['l_shoulder_x'], row['l_shoulder_y']]),\n",
    "        np.array([row['l_elbow_x'], row['l_elbow_y']]),\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_shoulder_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['r_shoulder_x'], row['r_shoulder_y']]),\n",
    "        np.array([row['r_elbow_x'], row['r_elbow_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_shoulder_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['l_shoulder_x'], row['l_shoulder_y']]),\n",
    "        np.array([row['l_elbow_x'], row['l_elbow_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_knee_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['r_hip_x'], row['r_hip_y']]),\n",
    "        np.array([row['r_knee_x'], row['r_knee_y']]),\n",
    "        np.array([row['r_ankle_x'], row['r_ankle_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_knee_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['l_hip_x'], row['l_hip_y']]),\n",
    "        np.array([row['l_knee_x'], row['l_knee_y']]),\n",
    "        np.array([row['l_ankle_x'], row['l_ankle_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_hip_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "        np.array([row['r_hip_x'], row['r_hip_y']]),\n",
    "        np.array([row['r_knee_x'], row['r_knee_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_hip_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "        np.array([row['l_hip_x'], row['l_hip_y']]),\n",
    "        np.array([row['l_knee_x'], row['l_knee_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['trunk_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['nose_x'], row['nose_y']]),\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    # === DISTANCES ===\n",
    "    df['eye_to_eye'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_eye_x'], row['l_eye_y']]),\n",
    "        np.array([row['r_eye_x'], row['r_eye_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['nose_to_neck'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['nose_x'], row['nose_y']]),\n",
    "        np.array([row['neck_x'], row['neck_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_wrist_to_hip'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_wrist_to_hip'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_wrist_to_nose'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']]),\n",
    "        np.array([row['nose_x'], row['nose_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_wrist_to_nose'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']]),\n",
    "        np.array([row['nose_x'], row['nose_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['nose_to_ankles'] = df.apply(lambda row: (\n",
    "        compute_distance(np.array([row['nose_x'], row['nose_y']]),\n",
    "                         np.array([row['l_ankle_x'], row['l_ankle_y']])) +\n",
    "        compute_distance(np.array([row['nose_x'], row['nose_y']]),\n",
    "                         np.array([row['r_ankle_x'], row['r_ankle_y']]))\n",
    "    ) / 2, axis=1)\n",
    "\n",
    "    df['hip_to_ankle'] = df.apply(lambda row: (\n",
    "        compute_distance(np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "                         np.array([row['l_ankle_x'], row['l_ankle_y']])) +\n",
    "        compute_distance(np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "                         np.array([row['r_ankle_x'], row['r_ankle_y']]))\n",
    "    ) / 2, axis=1)\n",
    "\n",
    "    # === SYMMETRY FEATURES ===\n",
    "    df['shoulder_y_diff'] = df['l_shoulder_y'] - df['r_shoulder_y']\n",
    "    df['hip_y_diff'] = df['l_hip_y'] - df['r_hip_y']\n",
    "    df['elbow_angle_diff'] = df['l_elbow_angle'] - df['r_elbow_angle']\n",
    "    df['knee_angle_diff'] = df['l_knee_angle'] - df['r_knee_angle']\n",
    "    df['wrist_to_hip_diff'] = df['l_wrist_to_hip'] - df['r_wrist_to_hip']\n",
    "    df['shoulder_angle_diff'] = df['l_shoulder_angle'] - df['r_shoulder_angle']\n",
    "\n",
    "    df['com_x'] = (df['mid_hip_x'] + df['neck_x']) / 2\n",
    "    df['com_y'] = (df['mid_hip_y'] + df['neck_y']) / 2\n",
    "    \n",
    "    df['body_spread_x'] = df[['l_wrist_x', 'r_wrist_x', 'l_ankle_x', 'r_ankle_x']].max(axis=1) - \\\n",
    "                          df[['l_wrist_x', 'r_wrist_x', 'l_ankle_x', 'r_ankle_x']].min(axis=1)\n",
    "    df['body_spread_y'] = df[['nose_y', 'l_ankle_y', 'r_ankle_y']].max(axis=1) - \\\n",
    "                          df[['nose_y', 'l_ankle_y', 'r_ankle_y']].min(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_temporal_features_fixed(df, keypoints, fps=15):\n",
    "    \"\"\"\n",
    "    FIXED: Proper velocity and acceleration calculations\n",
    "    - Velocity = change in position / change in time (units/second)\n",
    "    - Acceleration = change in velocity / change in time (units/second²)\n",
    "    - No zero-filling for NaNs, they will be handled later with proper imputation\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['person_label', 'time_s']).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate time differences properly\n",
    "    df['delta_time'] = df.groupby('person_label')['time_s'].diff()\n",
    "    \n",
    "    # For first frame in each sequence, use median frame time\n",
    "    median_frame_time = 1 / fps\n",
    "    df['delta_time'] = df['delta_time'].fillna(median_frame_time)\n",
    "    \n",
    "    # Clip extremely small values to avoid division issues\n",
    "    df['delta_time'] = df['delta_time'].clip(lower=0.001)\n",
    "    \n",
    "    # Select keypoints for velocity/acceleration\n",
    "    selected_keypoints = [\n",
    "        'com_x', 'com_y', 'nose_x', 'nose_y',\n",
    "        'l_wrist_x', 'l_wrist_y', 'r_wrist_x', 'r_wrist_y',\n",
    "        'l_ankle_x', 'l_ankle_y', 'r_ankle_x', 'r_ankle_y',\n",
    "        'neck_x', 'neck_y', 'mid_hip_x', 'mid_hip_y'\n",
    "    ]\n",
    "    \n",
    "    velocity_features = []\n",
    "    acceleration_features = []\n",
    "    \n",
    "    for col in selected_keypoints:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # VELOCITY: (position_t - position_t-1) / delta_time\n",
    "        position_diff = df.groupby('person_label')[col].diff()\n",
    "        velocity = position_diff / df['delta_time']\n",
    "        vel_col = f'{col}_vel'\n",
    "        df[vel_col] = velocity\n",
    "        velocity_features.append(vel_col)\n",
    "        \n",
    "        # ACCELERATION: (velocity_t - velocity_t-1) / delta_time\n",
    "        velocity_diff = df.groupby('person_label')[vel_col].diff()\n",
    "        acceleration = velocity_diff / df['delta_time']\n",
    "        acc_col = f'{col}_acc'\n",
    "        df[acc_col] = acceleration\n",
    "        acceleration_features.append(acc_col)\n",
    "    \n",
    "    # Remove infinite values (but keep NaN for proper imputation later)\n",
    "    for col in velocity_features + acceleration_features:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    print(f\"✓ Added {len(velocity_features)} velocity and {len(acceleration_features)} acceleration features\")\n",
    "    \n",
    "    return df, velocity_features + acceleration_features\n",
    "\n",
    "\n",
    "def normalize_keypoints(df, keypoints):\n",
    "    \"\"\"Normalize keypoints relative to torso length\"\"\"\n",
    "    df['torso_length'] = np.sqrt(\n",
    "        (df['neck_x'] - df['mid_hip_x'])**2 + \n",
    "        (df['neck_y'] - df['mid_hip_y'])**2\n",
    "    )\n",
    "    \n",
    "    # Handle zero/invalid torso lengths\n",
    "    df['torso_length'] = df['torso_length'].replace(0, np.nan)\n",
    "    median_torso = df['torso_length'].median()\n",
    "    df['torso_length'] = df['torso_length'].fillna(median_torso)\n",
    "    df['torso_length'] = df['torso_length'].clip(lower=1e-3)\n",
    "\n",
    "    for col in keypoints:\n",
    "        if '_x' in col:\n",
    "            df[col] = (df[col] - df['mid_hip_x']) / df['torso_length']\n",
    "        else:\n",
    "            df[col] = (df[col] - df['mid_hip_y']) / df['torso_length']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56308580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering ---\n",
      "✓ Found 58 keypoint columns\n",
      "✓ Added 16 velocity and 16 acceleration features\n"
     ]
    }
   ],
   "source": [
    "#  Feature engineering\n",
    "print(\"\\n--- Feature Engineering ---\")\n",
    "# Identify keypoints\n",
    "keypoints = [col for col in data.columns if '_x' in col or '_y' in col]\n",
    "print(f\"✓ Found {len(keypoints)} keypoint columns\")\n",
    "    \n",
    "df = normalize_keypoints(data, keypoints)\n",
    "df = add_derived_pose_features(df)\n",
    "df, temporal_features = add_temporal_features_fixed(df, keypoints)\n",
    "df['person_Child'] = (df['person_label'] == 'Child').astype(int)\n",
    "\n",
    "# Select only the features used in the model\n",
    "missing_cols = [f for f in feature_names if f not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The following required features are missing in the data: {missing_cols}\")\n",
    "\n",
    "X = df[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0e939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Predictions saved to: /home/liubov/Bureau/new/output_data/predictions_processed_data14-3-2024_#15_INDIVIDUAL_[18].csv\n",
      " No true labels found in CSV; skipping performance report.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Predict annotation labels\n",
    "# =========================\n",
    "predictions = model.predict(X)\n",
    "data['predicted_annotation_label'] = predictions\n",
    "\n",
    "# Save predictions\n",
    "predictions_file = OUTPUT_BASE_DIR / f\"predictions_{NEW_DATA_FILE.stem}.csv\"\n",
    "data.to_csv(predictions_file, index=False)\n",
    "print(f\"✓ Predictions saved to: {predictions_file}\")\n",
    "\n",
    "# =========================\n",
    "# Performance report (if true labels exist)\n",
    "# =========================\n",
    "if 'annotation_label' in data.columns:\n",
    "    y_true = data['annotation_label']\n",
    "    y_pred = predictions\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Save textual report\n",
    "    report_file = OUTPUT_BASE_DIR / f\"prediction_report_{NEW_DATA_FILE.stem}.txt\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"✓ Performance report saved to: {report_file}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis',\n",
    "                xticklabels=np.unique(y_true),\n",
    "                yticklabels=np.unique(y_true))\n",
    "    plt.title(\"Confusion Matrix\", fontsize=20, weight='bold')\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=16, weight='bold')\n",
    "    plt.ylabel(\"True Label\", fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    cm_file = OUTPUT_BASE_DIR / f\"confusion_matrix_{NEW_DATA_FILE.stem}.png\"\n",
    "    plt.savefig(cm_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"✓ Confusion matrix saved to: {cm_file}\")\n",
    "\n",
    "else:\n",
    "    print(\" No true labels found in CSV; skipping performance report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf98fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.read_csv('/home/liubov/Bureau/new/output_data/predictions_processed_data14-3-2024_#15_INDIVIDUAL_[18].csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afb23eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_annotation_label\n",
       "9     11231\n",
       "12     9419\n",
       "2      8512\n",
       "3      8012\n",
       "5      3373\n",
       "4      2478\n",
       "1      2004\n",
       "10      893\n",
       "0       444\n",
       "6       204\n",
       "11      160\n",
       "8        49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df['predicted_annotation_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "0:'C', 1:'CCR', 2:'CHO', 3:'CSI', 4:'CST', 5:'T', 6:'TC', 7:'TRE', 8:'TSI', 9:'TST', 10, 11, 12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
