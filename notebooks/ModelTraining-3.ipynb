{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a3ecfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "IMPROVED MODEL TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "✓ Loaded data: 46421 rows, 57 columns\n",
      "✓ Filtered dataset size: 46405\n",
      "✓ Kept 12 classes with >= 30 samples\n",
      "✓ Found 52 keypoint columns\n",
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE CHECKS\n",
      "======================================================================\n",
      "\n",
      "1. Time distribution per class:\n",
      "                         min          max  count\n",
      "annotation_label                                \n",
      "C                 822.933333  1784.466667     44\n",
      "CCR               565.066667   627.866667    943\n",
      "CHO               165.000000  1082.400000   2716\n",
      "CSI               627.933333  1784.133333   7371\n",
      "CST               253.066667  1846.866667  10246\n",
      "T                 180.533333  1517.066667   3802\n",
      "TC                210.000000  1691.133333    252\n",
      "TRE               529.933333   533.000000     47\n",
      "TSI               241.600000  1759.666667  11432\n",
      "TST               165.000000  1524.133333   3233\n",
      "⚠ WARNING: 1 class pairs don't overlap in time:\n",
      "  - CSI and CCR\n",
      "  This could indicate temporal leakage if train/test split is not time-aware\n",
      "\n",
      "2. Feature-target correlation: Skipped (target is not numeric or no features available)\n",
      "\n",
      "✓ No duplicated rows\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Feature Engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ✓ Added 16 velocity and 16 acceleration features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded labels: 12 classes\n",
      "  Classes: ['C' 'CCR' 'CHO' 'CSI' 'CST' 'T' 'TC' 'TRE' 'TSI' 'TST']...\n",
      "\n",
      "--- Preparing Features ---\n",
      "Columns before cleanup: 120\n",
      "Sample columns: ['person_label', 'time_s', 'avg_pose_conf', 'wrist_dist', 'nose_x', 'nose_y', 'neck_x', 'neck_y', 'r_shoulder_x', 'r_shoulder_y']\n",
      "✓ Converting person_label to dummy variables\n",
      "Dropping metadata columns: ['time_s', 'avg_pose_conf', 'annotation_label', 'label_class', 'delta_time', 'torso_length', 'label_encoded']\n",
      "✓ Feature matrix shape: (46405, 114)\n",
      "✓ Target shape: (46405,)\n",
      "✓ Number of features: 114\n",
      "✓ Number of samples: 46405\n",
      "\n",
      "--- Handling Missing Values ---\n",
      "NaN values before imputation: 576\n",
      "NaN values after imputation: 0\n",
      "✓ Imputed 576 missing values\n",
      "\n",
      "======================================================================\n",
      "FEATURE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Initial features: 114\n",
      "\n",
      "1. Removing zero-variance features...\n",
      "   Found 6 zero-variance features\n",
      "\n",
      "2. Analyzing feature-target correlations...\n",
      "   Top 10 most correlated features:\n",
      "   - person_Therapist: 0.8610\n",
      "   - person_Child: 0.8610\n",
      "   - neck_x: 0.7929\n",
      "   - com_x: 0.7929\n",
      "   - r_wrist_x: 0.7840\n",
      "   - r_shoulder_x: 0.7744\n",
      "   - l_shoulder_x: 0.7677\n",
      "   - nose_x: 0.7646\n",
      "   - r_elbow_x: 0.7418\n",
      "   - l_elbow_x: 0.7363\n",
      "\n",
      "   Removing 29 features with correlation < 0.01\n",
      "\n",
      "3. Removing redundant features (multicollinearity)...\n",
      "   Found 51 highly correlated features (>0.95)\n",
      "   Dropping 24 redundant features\n",
      "\n",
      "Final features: 55 (removed 53)\n",
      "======================================================================\n",
      "\n",
      "--- Train/Test Split Strategy ---\n",
      "Using STRATIFIED RANDOM split...\n",
      "✓ Train/test split: 37124/9281 samples\n",
      "\n",
      "======================================================================\n",
      "TRAINING TOP 3 MODELS (Handle Many Features Well)\n",
      "Using strong regularization and early stopping where supported\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training LightGBM with early stopping...\n",
      "  Optimal iterations: 300\n",
      "✓ LightGBM trained (using 300 iterations)\n",
      "\n",
      "[2/3] Training XGBoost with hyperparameter tuning...\n",
      "   Running randomized search (50 iterations, 3-fold CV)...\n",
      "   Best parameters found:\n",
      "     subsample: 0.7\n",
      "     reg_lambda: 1.5\n",
      "     reg_alpha: 1.0\n",
      "     n_estimators: 200\n",
      "     min_child_weight: 7\n",
      "     max_depth: 6\n",
      "     learning_rate: 0.1\n",
      "     gamma: 0.3\n",
      "     colsample_bytree: 0.6\n",
      "   Best CV F1 score: 0.9181\n",
      "✓ XGBoost trained with optimized hyperparameters\n",
      "\n",
      "[3/3] Training HistGradientBoosting with early stopping...\n",
      "✓ HistGradientBoosting trained with early stopping\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODELS\n",
      "======================================================================\n",
      "\n",
      "--- LightGBM ---\n",
      "Test  - F1: 0.9257, ROC-AUC: 0.9963\n",
      "\n",
      "--- Overfitting Analysis: LightGBM ---\n",
      "Train F1: 0.9739\n",
      "Test F1:  0.9257\n",
      "Gap:      0.0482\n",
      "✓ No significant overfitting\n",
      "\n",
      "--- XGBoost ---\n",
      "Test  - F1: 0.9280, ROC-AUC: 0.9966\n",
      "\n",
      "--- Overfitting Analysis: XGBoost ---\n",
      "Train F1: 0.9806\n",
      "Test F1:  0.9280\n",
      "Gap:      0.0525\n",
      "✓ No significant overfitting\n",
      "\n",
      "--- HistGradientBoosting ---\n",
      "Test  - F1: 0.9263, ROC-AUC: 0.9958\n",
      "\n",
      "--- Overfitting Analysis: HistGradientBoosting ---\n",
      "Train F1: 0.9781\n",
      "Test F1:  0.9263\n",
      "Gap:      0.0518\n",
      "✓ No significant overfitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ✓ Saved: model_comparison.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ✓ Saved: feature_importance.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION CHECK (3-Fold, Best Model Only)\n",
      "======================================================================\n",
      "\n",
      "Top 15 most important features (XGBoost):\n",
      "  person_Child                   0.3197\n",
      "  person_Therapist               0.1734\n",
      "  r_wrist_x                      0.0337\n",
      "  nose_to_neck                   0.0237\n",
      "  r_hip_y                        0.0227\n",
      "  nose_x                         0.0195\n",
      "  r_heel_x                       0.0182\n",
      "  nose_y                         0.0154\n",
      "  r_small_toe_x                  0.0148\n",
      "  l_wrist_y                      0.0136\n",
      "  body_spread_x                  0.0126\n",
      "  l_knee_x                       0.0124\n",
      "  r_ankle_y                      0.0120\n",
      "  l_big_toe_y                    0.0118\n",
      "  neck_x                         0.0116\n",
      "======================================================================\n",
      "Running CV on best model: XGBoost\n",
      "XGBoost: CV F1 = 0.9181 (+/- 0.0014)\n",
      "Individual fold scores: ['0.9198', '0.9163', '0.9182']\n",
      "✓ Low variance in CV scores - model is stable\n",
      "\n",
      "======================================================================\n",
      "BEST MODEL: XGBoost\n",
      "Test F1 Score: 0.9280\n",
      "Overfitting Status: good\n",
      "======================================================================\n",
      "\n",
      "✓ Saved all models and artifacts to /home/liubov/Bureau/new/output_data\n",
      "\n",
      "✓ Training complete!\n",
      "✓ Best model: XGBoost\n",
      "✓ Final F1 Score: 0.9280\n",
      "✓ Overfitting status: good\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved Feature Selection and Model Training Pipeline\n",
    "======================================\n",
    "IMPROVEMENTS:\n",
    "1. Fixed velocity and acceleration calculations (proper time-based derivatives)\n",
    "2. Removed PCA - keeping all features for models that handle high dimensionality\n",
    "3. Better NaN handling (median imputation instead of zero-filling)\n",
    "4. Using top 3 models that handle many features: LightGBM, XGBoost, HistGradientBoosting\n",
    "5. Added data leakage checks and overfitting detection\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import joblib  \n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS \n",
    "# =============================================================================\n",
    "\n",
    "def add_derived_pose_features(df):\n",
    "    \"\"\"\n",
    "    Add comprehensive joint angles, distances, and symmetry features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_angle(a, b, c):\n",
    "        \"\"\"Compute angle at point b formed by points a-b-c\"\"\"\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
    "        return np.arccos(np.clip(cosine_angle, -1.0, 1.0)) * (180.0 / np.pi)\n",
    "\n",
    "    def compute_distance(p1, p2):\n",
    "        \"\"\"Euclidean distance between two points\"\"\"\n",
    "        return np.linalg.norm(p1 - p2)\n",
    "\n",
    "    # === JOINT ANGLES ===\n",
    "    df['r_elbow_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['r_shoulder_x'], row['r_shoulder_y']]),\n",
    "        np.array([row['r_elbow_x'], row['r_elbow_y']]),\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_elbow_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['l_shoulder_x'], row['l_shoulder_y']]),\n",
    "        np.array([row['l_elbow_x'], row['l_elbow_y']]),\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_shoulder_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['r_shoulder_x'], row['r_shoulder_y']]),\n",
    "        np.array([row['r_elbow_x'], row['r_elbow_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_shoulder_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['l_shoulder_x'], row['l_shoulder_y']]),\n",
    "        np.array([row['l_elbow_x'], row['l_elbow_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_knee_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['r_hip_x'], row['r_hip_y']]),\n",
    "        np.array([row['r_knee_x'], row['r_knee_y']]),\n",
    "        np.array([row['r_ankle_x'], row['r_ankle_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_knee_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['l_hip_x'], row['l_hip_y']]),\n",
    "        np.array([row['l_knee_x'], row['l_knee_y']]),\n",
    "        np.array([row['l_ankle_x'], row['l_ankle_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_hip_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "        np.array([row['r_hip_x'], row['r_hip_y']]),\n",
    "        np.array([row['r_knee_x'], row['r_knee_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_hip_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "        np.array([row['l_hip_x'], row['l_hip_y']]),\n",
    "        np.array([row['l_knee_x'], row['l_knee_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['trunk_angle'] = df.apply(lambda row: compute_angle(\n",
    "        np.array([row['nose_x'], row['nose_y']]),\n",
    "        np.array([row['neck_x'], row['neck_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    # === DISTANCES ===\n",
    "    df['eye_to_eye'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_eye_x'], row['l_eye_y']]),\n",
    "        np.array([row['r_eye_x'], row['r_eye_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['nose_to_neck'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['nose_x'], row['nose_y']]),\n",
    "        np.array([row['neck_x'], row['neck_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_wrist_to_hip'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_wrist_to_hip'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']]),\n",
    "        np.array([row['mid_hip_x'], row['mid_hip_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['r_wrist_to_nose'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['r_wrist_x'], row['r_wrist_y']]),\n",
    "        np.array([row['nose_x'], row['nose_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['l_wrist_to_nose'] = df.apply(lambda row: compute_distance(\n",
    "        np.array([row['l_wrist_x'], row['l_wrist_y']]),\n",
    "        np.array([row['nose_x'], row['nose_y']])\n",
    "    ), axis=1)\n",
    "\n",
    "    df['nose_to_ankles'] = df.apply(lambda row: (\n",
    "        compute_distance(np.array([row['nose_x'], row['nose_y']]),\n",
    "                         np.array([row['l_ankle_x'], row['l_ankle_y']])) +\n",
    "        compute_distance(np.array([row['nose_x'], row['nose_y']]),\n",
    "                         np.array([row['r_ankle_x'], row['r_ankle_y']]))\n",
    "    ) / 2, axis=1)\n",
    "\n",
    "    df['hip_to_ankle'] = df.apply(lambda row: (\n",
    "        compute_distance(np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "                         np.array([row['l_ankle_x'], row['l_ankle_y']])) +\n",
    "        compute_distance(np.array([row['mid_hip_x'], row['mid_hip_y']]),\n",
    "                         np.array([row['r_ankle_x'], row['r_ankle_y']]))\n",
    "    ) / 2, axis=1)\n",
    "\n",
    "    # === SYMMETRY FEATURES ===\n",
    "    df['shoulder_y_diff'] = df['l_shoulder_y'] - df['r_shoulder_y']\n",
    "    df['hip_y_diff'] = df['l_hip_y'] - df['r_hip_y']\n",
    "    df['elbow_angle_diff'] = df['l_elbow_angle'] - df['r_elbow_angle']\n",
    "    df['knee_angle_diff'] = df['l_knee_angle'] - df['r_knee_angle']\n",
    "    df['wrist_to_hip_diff'] = df['l_wrist_to_hip'] - df['r_wrist_to_hip']\n",
    "    df['shoulder_angle_diff'] = df['l_shoulder_angle'] - df['r_shoulder_angle']\n",
    "\n",
    "    df['com_x'] = (df['mid_hip_x'] + df['neck_x']) / 2\n",
    "    df['com_y'] = (df['mid_hip_y'] + df['neck_y']) / 2\n",
    "    \n",
    "    df['body_spread_x'] = df[['l_wrist_x', 'r_wrist_x', 'l_ankle_x', 'r_ankle_x']].max(axis=1) - \\\n",
    "                          df[['l_wrist_x', 'r_wrist_x', 'l_ankle_x', 'r_ankle_x']].min(axis=1)\n",
    "    df['body_spread_y'] = df[['nose_y', 'l_ankle_y', 'r_ankle_y']].max(axis=1) - \\\n",
    "                          df[['nose_y', 'l_ankle_y', 'r_ankle_y']].min(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_temporal_features_fixed(df, keypoints, fps=15):\n",
    "    \"\"\"\n",
    "    FIXED: Proper velocity and acceleration calculations\n",
    "    - Velocity = change in position / change in time (units/second)\n",
    "    - Acceleration = change in velocity / change in time (units/second²)\n",
    "    - No zero-filling for NaNs, they will be handled later with proper imputation\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['annotation_label', 'time_s']).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate time differences properly\n",
    "    df['delta_time'] = df.groupby('annotation_label')['time_s'].diff()\n",
    "    \n",
    "    # For first frame in each sequence, use median frame time\n",
    "    median_frame_time = 1 / fps\n",
    "    df['delta_time'] = df['delta_time'].fillna(median_frame_time)\n",
    "    \n",
    "    # Clip extremely small values to avoid division issues\n",
    "    df['delta_time'] = df['delta_time'].clip(lower=0.001)\n",
    "    \n",
    "    # Select keypoints for velocity/acceleration\n",
    "    selected_keypoints = [\n",
    "        'com_x', 'com_y', 'nose_x', 'nose_y',\n",
    "        'l_wrist_x', 'l_wrist_y', 'r_wrist_x', 'r_wrist_y',\n",
    "        'l_ankle_x', 'l_ankle_y', 'r_ankle_x', 'r_ankle_y',\n",
    "        'neck_x', 'neck_y', 'mid_hip_x', 'mid_hip_y'\n",
    "    ]\n",
    "    \n",
    "    velocity_features = []\n",
    "    acceleration_features = []\n",
    "    \n",
    "    for col in selected_keypoints:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # VELOCITY: (position_t - position_t-1) / delta_time\n",
    "        position_diff = df.groupby('annotation_label')[col].diff()\n",
    "        velocity = position_diff / df['delta_time']\n",
    "        vel_col = f'{col}_vel'\n",
    "        df[vel_col] = velocity\n",
    "        velocity_features.append(vel_col)\n",
    "        \n",
    "        # ACCELERATION: (velocity_t - velocity_t-1) / delta_time\n",
    "        velocity_diff = df.groupby('annotation_label')[vel_col].diff()\n",
    "        acceleration = velocity_diff / df['delta_time']\n",
    "        acc_col = f'{col}_acc'\n",
    "        df[acc_col] = acceleration\n",
    "        acceleration_features.append(acc_col)\n",
    "    \n",
    "    # Remove infinite values (but keep NaN for proper imputation later)\n",
    "    for col in velocity_features + acceleration_features:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    logger.info(f\"✓ Added {len(velocity_features)} velocity and {len(acceleration_features)} acceleration features\")\n",
    "    \n",
    "    return df, velocity_features + acceleration_features\n",
    "\n",
    "\n",
    "def normalize_keypoints(df, keypoints):\n",
    "    \"\"\"Normalize keypoints relative to torso length\"\"\"\n",
    "    df['torso_length'] = np.sqrt(\n",
    "        (df['neck_x'] - df['mid_hip_x'])**2 + \n",
    "        (df['neck_y'] - df['mid_hip_y'])**2\n",
    "    )\n",
    "    \n",
    "    # Handle zero/invalid torso lengths\n",
    "    df['torso_length'] = df['torso_length'].replace(0, np.nan)\n",
    "    median_torso = df['torso_length'].median()\n",
    "    df['torso_length'] = df['torso_length'].fillna(median_torso)\n",
    "    df['torso_length'] = df['torso_length'].clip(lower=1e-3)\n",
    "\n",
    "    for col in keypoints:\n",
    "        if '_x' in col:\n",
    "            df[col] = (df[col] - df['mid_hip_x']) / df['torso_length']\n",
    "        else:\n",
    "            df[col] = (df[col] - df['mid_hip_y']) / df['torso_length']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_data_leakage(df, target_col='label_encoded', label_col='annotation_label'):\n",
    "    \"\"\"\n",
    "    Check for potential data leakage issues\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA LEAKAGE CHECKS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check 1: Time-based leakage\n",
    "    if 'time_s' in df.columns and label_col in df.columns:\n",
    "        time_label_corr = df.groupby(label_col)['time_s'].agg(['min', 'max', 'count'])\n",
    "        print(\"\\n1. Time distribution per class:\")\n",
    "        print(time_label_corr.head(10))\n",
    "        \n",
    "        # Check if certain classes only appear in specific time ranges\n",
    "        # Only check classes with enough samples to be meaningful\n",
    "        classes = df[label_col].value_counts()\n",
    "        significant_classes = classes[classes >= 100].index.tolist()  # Only check classes with 100+ samples\n",
    "        \n",
    "        non_overlapping_pairs = []\n",
    "        if len(significant_classes) > 1:\n",
    "            for i, class1 in enumerate(significant_classes[:10]):  # Check first 10 significant classes\n",
    "                for class2 in significant_classes[i+1:10]:\n",
    "                    range1 = df[df[label_col] == class1]['time_s'].agg(['min', 'max'])\n",
    "                    range2 = df[df[label_col] == class2]['time_s'].agg(['min', 'max'])\n",
    "                    if range1['max'] < range2['min'] or range2['max'] < range1['min']:\n",
    "                        non_overlapping_pairs.append((class1, class2))\n",
    "        \n",
    "        if non_overlapping_pairs:\n",
    "            print(f\" WARNING: {len(non_overlapping_pairs)} class pairs don't overlap in time:\")\n",
    "            for c1, c2 in non_overlapping_pairs[:3]:  # Show first 3\n",
    "                print(f\"  - {c1} and {c2}\")\n",
    "            print(\"  This could indicate temporal leakage if train/test split is not time-aware\")\n",
    "        else:\n",
    "            print(\"✓ Significant classes show temporal overlap\")\n",
    "            \n",
    "    elif 'time_s' not in df.columns:\n",
    "        print(\"\\n1. Time-based check: Skipped (time_s not in dataframe)\")\n",
    "    else:\n",
    "        print(\"\\n1. Time-based check: Skipped (label column not found)\")\n",
    "    \n",
    "    # Check 2: Feature-target correlation (only if target is numeric)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_cols = [col for col in numeric_cols if col != target_col and 'time' not in col]\n",
    "    \n",
    "    # Only check correlation if target is numeric\n",
    "    if target_col in numeric_cols and len(feature_cols) > 0:\n",
    "        print(\"\\n2. Feature-target correlation check:\")\n",
    "        correlations = df[feature_cols + [target_col]].corr()[target_col].drop(target_col).abs()\n",
    "        high_corr_features = correlations[correlations > 0.95]\n",
    "        \n",
    "        if len(high_corr_features) > 0:\n",
    "            print(f\" WARNING: {len(high_corr_features)} features have suspiciously high correlation (>0.95) with target:\")\n",
    "            print(high_corr_features.head(10))\n",
    "        else:\n",
    "            print(\"✓ No features with suspiciously high correlation to target\")\n",
    "    else:\n",
    "        print(\"\\n2. Feature-target correlation: Skipped (target is not numeric or no features available)\")\n",
    "    \n",
    "    # Check 3: Duplicated rows\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        print(f\"\\n WARNING: {dup_count} duplicated rows found - could indicate leakage\")\n",
    "    else:\n",
    "        print(\"\\n✓ No duplicated rows\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def detect_overfitting(train_scores, test_scores, model_name, threshold=0.10):\n",
    "    \"\"\"\n",
    "    Detect overfitting by comparing train vs test performance\n",
    "    \"\"\"\n",
    "    gap = train_scores['f1'] - test_scores['f1']\n",
    "    \n",
    "    print(f\"\\n--- Overfitting Analysis: {model_name} ---\")\n",
    "    print(f\"Train F1: {train_scores['f1']:.4f}\")\n",
    "    print(f\"Test F1:  {test_scores['f1']:.4f}\")\n",
    "    print(f\"Gap:      {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.15:\n",
    "        print(\" SEVERE OVERFITTING detected!\")\n",
    "        return 'severe'\n",
    "    elif gap > threshold:\n",
    "        print(\" Moderate overfitting detected\")\n",
    "        return 'moderate'\n",
    "    else:\n",
    "        print(\" No significant overfitting\")\n",
    "        return 'good'\n",
    "\n",
    "\n",
    "def plot_model_comparison(test_metrics_dict, train_metrics_dict, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create comparison plot for models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    test_df = pd.DataFrame(test_metrics_dict).T\n",
    "    train_df = pd.DataFrame(train_metrics_dict).T\n",
    "    \n",
    "    # Test metrics\n",
    "    test_df.plot(kind='bar', ax=axes[0], edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "    axes[0].set_title('Test Set Performance', fontsize=14, weight='bold')\n",
    "    axes[0].set_ylabel('Score', fontsize=12)\n",
    "    axes[0].set_xlabel('Model', fontsize=12)\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "    axes[0].legend(title='Metric', loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim([0, 1.05])\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap_df = train_df - test_df\n",
    "    gap_df['f1'].plot(kind='bar', ax=axes[1], color='coral', edgecolor='black', linewidth=1.5)\n",
    "    axes[1].axhline(y=0.05, color='orange', linestyle='--', linewidth=2, label='Acceptable (0.05)')\n",
    "    axes[1].axhline(y=0.10, color='red', linestyle='--', linewidth=2, label='Warning (0.10)')\n",
    "    axes[1].set_title('Train-Test F1 Gap', fontsize=14, weight='bold')\n",
    "    axes[1].set_ylabel('F1 Gap', fontsize=12)\n",
    "    axes[1].set_xlabel('Model', fontsize=12)\n",
    "    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(Path(save_dir) / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        logger.info(\"✓ Saved: model_comparison.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main(results, output_dir=None):\n",
    "    \"\"\"\n",
    "    Improved training pipeline with:\n",
    "    - Top 3 models for high-dimensional data: LightGBM, XGBoost, HistGradientBoosting\n",
    "    - No PCA (these models handle many features well)\n",
    "    - Proper NaN handling with median imputation\n",
    "    - Data leakage and overfitting checks\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"IMPROVED MODEL TRAINING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    df = results['labeled_features'].copy()\n",
    "    print(f\"\\n✓ Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Map labels to classes\n",
    "    label_to_class_map = {\n",
    "        'CS': 'vocalizations', 'CNS': 'vocalizations', 'TS': 'vocalizations', 'TNS': 'vocalizations',\n",
    "        'CGO': 'unlabeled', 'TGO': 'unlabeled',\n",
    "        'TC': 'joint_attention', 'AO': 'joint_attention', 'AT': 'joint_attention',\n",
    "        'GO': 'joint_attention', 'GT': 'joint_attention',\n",
    "        'T': 'interactions', 'T_V': 'interactions', 'T_P': 'interactions', 'C': 'interactions',\n",
    "        'TST': 'coordination', 'THO': 'coordination', 'TSI': 'coordination', 'TLF': 'coordination',\n",
    "        'TRE': 'coordination', 'TCR': 'coordination', 'CST': 'coordination', 'CHO': 'coordination',\n",
    "        'CSI': 'coordination', 'CLF': 'coordination', 'CRE': 'coordination', 'CCR': 'coordination'\n",
    "    }\n",
    "\n",
    "    df['label_class'] = df['annotation_label'].map(label_to_class_map)\n",
    "    non_posture_classes = ['vocalizations', 'unlabeled']\n",
    "    df = df[~df['label_class'].isin(non_posture_classes)]\n",
    "    print(f\"✓ Filtered dataset size: {df.shape[0]}\")\n",
    "    \n",
    "    # Filter classes with minimum samples\n",
    "    min_samples = 30\n",
    "    class_counts = df['annotation_label'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= min_samples].index.tolist()\n",
    "    df = df[df['annotation_label'].isin(valid_classes)]\n",
    "    print(f\"✓ Kept {len(valid_classes)} classes with >= {min_samples} samples\")\n",
    "\n",
    "    # Identify keypoints\n",
    "    keypoints = [col for col in df.columns if '_x' in col or '_y' in col]\n",
    "    print(f\"✓ Found {len(keypoints)} keypoint columns\")\n",
    "    \n",
    "    # Check for data leakage BEFORE encoding (while annotation_label still exists)\n",
    "    check_data_leakage(df, target_col='annotation_label', label_col='annotation_label')\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\n--- Feature Engineering ---\")\n",
    "    df = normalize_keypoints(df, keypoints)\n",
    "    df = add_derived_pose_features(df)\n",
    "    df, temporal_features = add_temporal_features_fixed(df, keypoints)\n",
    "\n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['annotation_label'])\n",
    "    print(f\"✓ Encoded labels: {len(label_encoder.classes_)} classes\")\n",
    "    print(f\"  Classes: {label_encoder.classes_[:10]}{'...' if len(label_encoder.classes_) > 10 else ''}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    print(\"\\n--- Preparing Features ---\")\n",
    "    print(f\"Columns before cleanup: {df.shape[1]}\")\n",
    "    print(f\"Sample columns: {list(df.columns[:10])}\")\n",
    "    \n",
    "    # Save label_encoded before any operations\n",
    "    if 'label_encoded' not in df.columns:\n",
    "        raise ValueError(\"label_encoded not found! Check previous steps.\")\n",
    "    y = df['label_encoded'].copy()\n",
    "    \n",
    "    # Handle person_label if exists (convert to dummy variables)\n",
    "    if 'person_label' in df.columns:\n",
    "        print(\"✓ Converting person_label to dummy variables\")\n",
    "        df = pd.get_dummies(df, columns=['person_label'], prefix='person')\n",
    "    \n",
    "    # List of metadata columns to drop (if they exist)\n",
    "    metadata_cols = ['time_s', 'avg_pose_conf', 'annotation_label', 'label_class', \n",
    "                     'frame', 'delta_time', 'torso_length', 'label_encoded']\n",
    "    \n",
    "    # Drop metadata columns that exist\n",
    "    cols_to_drop = [col for col in metadata_cols if col in df.columns]\n",
    "    if cols_to_drop:\n",
    "        print(f\"Dropping metadata columns: {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Drop any remaining non-numeric columns\n",
    "    non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if non_numeric_cols:\n",
    "        print(f\"Dropping non-numeric columns: {non_numeric_cols}\")\n",
    "        df = df.drop(columns=non_numeric_cols)\n",
    "    \n",
    "    # Now df contains only features, y contains the target\n",
    "    X = df\n",
    "    \n",
    "    print(f\"✓ Feature matrix shape: {X.shape}\")\n",
    "    print(f\"✓ Target shape: {y.shape}\")\n",
    "    print(f\"✓ Number of features: {X.shape[1]}\")\n",
    "    print(f\"✓ Number of samples: {X.shape[0]}\")\n",
    "    \n",
    "    # Handle NaNs with median imputation (NOT zero-filling)\n",
    "    print(\"\\n--- Handling Missing Values ---\")\n",
    "    nan_counts_before = X.isnull().sum().sum()\n",
    "    print(f\"NaN values before imputation: {nan_counts_before}\")\n",
    "    \n",
    "    # Use median imputation for robustness\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "    \n",
    "    nan_counts_after = X.isnull().sum().sum()\n",
    "    print(f\"NaN values after imputation: {nan_counts_after}\")\n",
    "    print(f\"✓ Imputed {nan_counts_before - nan_counts_after} missing values\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_count = np.isinf(X.values).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"⚠ Found {inf_count} infinite values, replacing with NaN and re-imputing\")\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "        X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # FEATURE ANALYSIS AND REDUNDANCY REMOVAL\n",
    "    # =============================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nInitial features: {X.shape[1]}\")\n",
    "    \n",
    "    # 1. Remove zero-variance features\n",
    "    print(\"\\n1. Removing zero-variance features...\")\n",
    "    variances = X.var()\n",
    "    zero_var_features = variances[variances < 1e-8].index.tolist()\n",
    "    if zero_var_features:\n",
    "        print(f\"   Found {len(zero_var_features)} zero-variance features\")\n",
    "        X = X.drop(columns=zero_var_features)\n",
    "    else:\n",
    "        print(\"   ✓ No zero-variance features\")\n",
    "    \n",
    "    # 2. Feature-target correlation analysis\n",
    "    print(\"\\n2. Analyzing feature-target correlations...\")\n",
    "    X_with_target = X.copy()\n",
    "    X_with_target['target'] = y.values\n",
    "    correlations = X_with_target.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"   Top 10 most correlated features:\")\n",
    "    for feat, corr in correlations.head(10).items():\n",
    "        print(f\"   - {feat}: {corr:.4f}\")\n",
    "    \n",
    "    # Remove features with very low correlation (likely noise)\n",
    "    low_corr_threshold = 0.01\n",
    "    low_corr_features = correlations[correlations < low_corr_threshold].index.tolist()\n",
    "    if low_corr_features:\n",
    "        print(f\"\\n   Removing {len(low_corr_features)} features with correlation < {low_corr_threshold}\")\n",
    "        X = X.drop(columns=low_corr_features)\n",
    "    \n",
    "    # 3. Remove highly correlated features (multicollinearity)\n",
    "    print(\"\\n3. Removing redundant features (multicollinearity)...\")\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    redundant_threshold = 0.95\n",
    "    redundant_features = [column for column in upper_triangle.columns \n",
    "                         if any(upper_triangle[column] > redundant_threshold)]\n",
    "    \n",
    "    if redundant_features:\n",
    "        print(f\"   Found {len(redundant_features)} highly correlated features (>{redundant_threshold})\")\n",
    "        # Keep the feature with higher correlation to target\n",
    "        features_to_drop = []\n",
    "        for feat in redundant_features:\n",
    "            # Find what it's correlated with\n",
    "            correlated_with = upper_triangle[feat][upper_triangle[feat] > redundant_threshold].index.tolist()\n",
    "            if correlated_with:\n",
    "                # Compare correlations with target\n",
    "                feat_corr = correlations.get(feat, 0)\n",
    "                other_corr = correlations.get(correlated_with[0], 0)\n",
    "                if feat_corr < other_corr and feat not in features_to_drop:\n",
    "                    features_to_drop.append(feat)\n",
    "        \n",
    "        if features_to_drop:\n",
    "            print(f\"   Dropping {len(features_to_drop)} redundant features\")\n",
    "            X = X.drop(columns=features_to_drop)\n",
    "    else:\n",
    "        print(\"   ✓ No highly correlated features found\")\n",
    "    \n",
    "    print(f\"\\nFinal features: {X.shape[1]} (removed {X_with_target.shape[1] - 1 - X.shape[1]})\")\n",
    "    \n",
    "    # Update y to match X indices\n",
    "    y = y.iloc[X.index]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TIME-AWARE TRAIN/TEST SPLIT TO PREVENT TEMPORAL LEAKAGE\n",
    "    # =============================================================================\n",
    "    print(\"\\n--- Train/Test Split Strategy ---\")\n",
    "    \n",
    "    # Check if we have time information for time-aware split\n",
    "    has_time_info = 'time_s' in results['labeled_features'].columns\n",
    "    \n",
    "    if has_time_info and len(results['labeled_features']) == len(X):\n",
    "        print(\"Using TIME-AWARE split to prevent temporal leakage...\")\n",
    "        # Use temporal ordering - train on earlier data, test on later data\n",
    "        # This is more realistic for real-world deployment\n",
    "        time_values = results['labeled_features']['time_s'].values[:len(X)]\n",
    "        time_threshold = np.percentile(time_values, 80)  # 80% train, 20% test\n",
    "        \n",
    "        train_mask = time_values <= time_threshold\n",
    "        test_mask = time_values > time_threshold\n",
    "        \n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        \n",
    "        print(f\"✓ Time-aware split: Train on data up to t={time_threshold:.2f}s\")\n",
    "        print(f\"  Train size: {len(X_train)} samples\")\n",
    "        print(f\"  Test size: {len(X_test)} samples\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        train_class_dist = pd.Series(y_train).value_counts()\n",
    "        test_class_dist = pd.Series(y_test).value_counts()\n",
    "        \n",
    "        # Ensure all classes are in both sets\n",
    "        missing_in_test = set(y_train.unique()) - set(y_test.unique())\n",
    "        missing_in_train = set(y_test.unique()) - set(y_train.unique())\n",
    "        \n",
    "        if missing_in_test or missing_in_train:\n",
    "            print(\"⚠ Some classes missing in train or test, falling back to stratified split\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, stratify=y, random_state=42\n",
    "            )\n",
    "            print(f\"✓ Stratified random split: {len(X_train)}/{len(X_test)} samples\")\n",
    "    else:\n",
    "        # Fallback to stratified random split\n",
    "        print(\"Using STRATIFIED RANDOM split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "        print(f\"✓ Train/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # MODEL TRAINING - TOP 3 MODELS FOR HIGH-DIMENSIONAL DATA\n",
    "    # =============================================================================\n",
    "    \n",
    "    models_dict = {}\n",
    "    train_metrics_dict = {}\n",
    "    test_metrics_dict = {}\n",
    "    overfitting_status = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING TOP 3 MODELS (Handle Many Features Well)\")\n",
    "    print(\"Using strong regularization and early stopping where supported\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. LIGHTGBM (Best for speed + high-dimensional data)\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n[1/3] Training LightGBM with early stopping...\")\n",
    "    \n",
    "    # Create validation set for early stopping\n",
    "    X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.15, stratify=y_train, random_state=42\n",
    "    )\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,           # Will stop early\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        min_child_samples=30,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        min_gain_to_split=0.01,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(\n",
    "        X_train_fit, y_train_fit,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Retrain on full training set with optimal number of iterations\n",
    "    best_iteration = lgb_model.best_iteration_\n",
    "    print(f\"  Optimal iterations: {best_iteration}\")\n",
    "    \n",
    "    lgb_model_final = lgb.LGBMClassifier(\n",
    "        n_estimators=best_iteration,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        min_child_samples=30,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        min_gain_to_split=0.01,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model_final.fit(X_train, y_train)\n",
    "    models_dict['LightGBM'] = lgb_model_final\n",
    "    print(f\"✓ LightGBM trained (using {best_iteration} iterations)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 2. XGBOOST (Excellent for structured/tabular data)\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n[2/3] Training XGBoost with hyperparameter tuning...\")\n",
    "    \n",
    "    # Define parameter search space\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "        'subsample': [0.6, 0.7, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "        'min_child_weight': [3, 5, 7],\n",
    "        'gamma': [0.1, 0.2, 0.3],\n",
    "        'reg_alpha': [0.5, 1.0, 1.5],\n",
    "        'reg_lambda': [0.5, 1.0, 1.5]\n",
    "    }\n",
    "    \n",
    "    # Base XGBoost model\n",
    "    xgb_base = XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    # Randomized search with 3-fold CV (faster than GridSearch)\n",
    "    print(\"   Running randomized search (50 iterations, 3-fold CV)...\")\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=50,              # Try 50 random combinations\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_random_search.fit(X_train, y_train)\n",
    "    xgb_model = xgb_random_search.best_estimator_\n",
    "    \n",
    "    print(f\"   Best parameters found:\")\n",
    "    for param, value in xgb_random_search.best_params_.items():\n",
    "        print(f\"     {param}: {value}\")\n",
    "    print(f\"   Best CV F1 score: {xgb_random_search.best_score_:.4f}\")\n",
    "    \n",
    "    models_dict['XGBoost'] = xgb_model\n",
    "    print(\"✓ XGBoost trained with optimized hyperparameters\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 3. HISTGRADIENTBOOSTING (sklearn's fast native option)\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n[3/3] Training HistGradientBoosting with early stopping...\")\n",
    "    hist_model = HistGradientBoostingClassifier(\n",
    "        max_iter=300,               # Will stop early via internal validation\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        l2_regularization=1.0,\n",
    "        min_samples_leaf=30,\n",
    "        max_bins=255,\n",
    "        max_leaf_nodes=31,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "        validation_fraction=0.15,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    hist_model.fit(X_train, y_train)\n",
    "    models_dict['HistGradientBoosting'] = hist_model\n",
    "    print(\"✓ HistGradientBoosting trained with early stopping\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # EVALUATION WITH OVERFITTING DETECTION\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATING MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        # Training metrics\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_prob = model.predict_proba(X_train)\n",
    "        \n",
    "        train_metrics = {\n",
    "            'f1': f1_score(y_train, train_pred, average='weighted'),\n",
    "            'precision': precision_score(y_train, train_pred, average='weighted'),\n",
    "            'recall': recall_score(y_train, train_pred, average='weighted'),\n",
    "            'roc_auc': roc_auc_score(y_train, train_pred_prob, multi_class='ovr', average='macro')\n",
    "        }\n",
    "        train_metrics_dict[model_name] = train_metrics\n",
    "        \n",
    "        # Test metrics\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_prob = model.predict_proba(X_test)\n",
    "        \n",
    "        test_metrics = {\n",
    "            'f1': f1_score(y_test, test_pred, average='weighted'),\n",
    "            'precision': precision_score(y_test, test_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, test_pred, average='weighted'),\n",
    "            'roc_auc': roc_auc_score(y_test, test_pred_prob, multi_class='ovr', average='macro')\n",
    "        }\n",
    "        test_metrics_dict[model_name] = test_metrics\n",
    "        \n",
    "        print(f\"Test  - F1: {test_metrics['f1']:.4f}, ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Detect overfitting\n",
    "        status = detect_overfitting(train_metrics, test_metrics, model_name)\n",
    "        overfitting_status[model_name] = status\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_model_comparison(test_metrics_dict, train_metrics_dict, save_dir=output_dir)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if output_dir:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        for idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "                indices = np.argsort(importances)[-20:]  # Top 20 features\n",
    "                \n",
    "                axes[idx].barh(range(len(indices)), importances[indices], color='steelblue')\n",
    "                axes[idx].set_yticks(range(len(indices)))\n",
    "                axes[idx].set_yticklabels([X.columns[i] for i in indices], fontsize=8)\n",
    "                axes[idx].set_xlabel('Importance', fontsize=10)\n",
    "                axes[idx].set_title(f'{model_name} - Top 20 Features', fontsize=12, weight='bold')\n",
    "                axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(output_dir) / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(\"✓ Saved: feature_importance.png\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Cross-validation check for best model (FAST VERSION - only best model, 3 folds)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CROSS-VALIDATION CHECK (3-Fold, Best Model Only)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find best model first\n",
    "    best_f1_pre = max([metrics['f1'] for metrics in test_metrics_dict.values()])\n",
    "    best_model_name_pre = [name for name, metrics in test_metrics_dict.items() if metrics['f1'] == best_f1_pre][0]\n",
    "    best_model_pre = models_dict[best_model_name_pre]\n",
    "    \n",
    "    # Print top features for best model\n",
    "    if hasattr(best_model_pre, 'feature_importances_'):\n",
    "        importances = best_model_pre.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 15 most important features ({best_model_name_pre}):\")\n",
    "        for idx, row in feature_importance_df.head(15).iterrows():\n",
    "            print(f\"  {row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Only run CV on the best model (much faster)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    print(f\"Running CV on best model: {best_model_name_pre}\")\n",
    "    cv_scores = cross_val_score(best_model_pre, X_train, y_train, cv=cv, \n",
    "                                scoring='f1_weighted', n_jobs=-1, verbose=0)\n",
    "    print(f\"{best_model_name_pre}: CV F1 = {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"Individual fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "    \n",
    "    # Check for overfitting via CV\n",
    "    if cv_scores.std() > 0.05:\n",
    "        print(\"⚠ High variance in CV scores - model may be unstable\")\n",
    "    else:\n",
    "        print(\"✓ Low variance in CV scores - model is stable\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_f1 = max([metrics['f1'] for metrics in test_metrics_dict.values()])\n",
    "    best_model_name = [name for name, metrics in test_metrics_dict.items() if metrics['f1'] == best_f1][0]\n",
    "    best_model = models_dict[best_model_name]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BEST MODEL: {best_model_name}\")\n",
    "    print(f\"Test F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"Overfitting Status: {overfitting_status[best_model_name]}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save artifacts\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save best model\n",
    "        filename = f\"model_{best_model_name.lower().replace(' ', '_')}.joblib\"\n",
    "        joblib.dump(best_model, output_dir / filename)\n",
    "        \n",
    "        # Save feature names\n",
    "        with open(output_dir / 'feature_names.json', 'w') as f:\n",
    "            json.dump(list(X.columns), f, indent=2)\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(output_dir / 'model_metrics.json', 'w') as f:\n",
    "            json.dump({\n",
    "                'train_metrics': {k: {kk: float(vv) for kk, vv in v.items()} \n",
    "                                for k, v in train_metrics_dict.items()},\n",
    "                'test_metrics': {k: {kk: float(vv) for kk, vv in v.items()} \n",
    "                               for k, v in test_metrics_dict.items()},\n",
    "                'overfitting_status': overfitting_status,\n",
    "                'best_model': best_model_name,\n",
    "                'n_features': X.shape[1],\n",
    "                'n_classes': len(label_encoder.classes_)\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n✓ Saved all models and artifacts to {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'models': models_dict,\n",
    "        'best_model': best_model,\n",
    "        'best_model_name': best_model_name,\n",
    "        'label_encoder': label_encoder,\n",
    "        'imputer': imputer,\n",
    "        'feature_names': list(X.columns),\n",
    "        'test_metrics': test_metrics_dict,\n",
    "        'train_metrics': train_metrics_dict,\n",
    "        'overfitting_status': overfitting_status,\n",
    "        'f1_score': best_f1\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    base_path = Path('/home/liubov/Bureau/new/output_data')\n",
    "    df = pd.read_csv('/home/liubov/Bureau/new/processed_data/labeled_features.csv', low_memory=False)\n",
    "    results = {'labeled_features': df}\n",
    "    \n",
    "    # Run training\n",
    "    model_package = main(results, output_dir=base_path)\n",
    "    \n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"✓ Best model: {model_package['best_model_name']}\")\n",
    "    print(f\"✓ Final F1 Score: {model_package['f1_score']:.4f}\")\n",
    "    print(f\"✓ Overfitting status: {model_package['overfitting_status'][model_package['best_model_name']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe89979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
